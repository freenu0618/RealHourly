import OpenAI from "openai";
import {
  buildChatContext,
  chatContextToPromptString,
  type ChatContext,
} from "./chat-context";
import { buildChatSystemPrompt } from "./chat-prompt";

let _openai: OpenAI | null = null;

function getOpenAI(): OpenAI {
  if (!_openai) {
    _openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY! });
  }
  return _openai;
}

export async function generateChatResponse(
  userId: string,
  userMessage: string,
  conversationHistory: { role: "user" | "assistant"; content: string }[],
): Promise<string> {
  const context = await buildChatContext(userId);

  if (context.totalActiveProjects === 0 && context.recentActivity.length === 0) {
    return "ì•„ì§ í”„ë¡œì íŠ¸ë‚˜ ì‘ì—… ê¸°ë¡ì´ ì—†ì–´ìš”. ë¨¼ì € í”„ë¡œì íŠ¸ë¥¼ ë§Œë“¤ê³  ì‹œê°„ì„ ê¸°ë¡í•´ë³´ì„¸ìš”! ğŸ“";
  }

  const contextString = chatContextToPromptString(context);
  const systemPrompt = buildChatSystemPrompt(contextString);

  // Keep last 5 conversation pairs (10 messages) to save token budget
  const recentHistory = conversationHistory.slice(-10);

  const primaryModel = process.env.LLM_MODEL_GENERATE || "gpt-4o-mini";
  const fallbackModel = "gpt-4o-mini";

  const messages: OpenAI.ChatCompletionMessageParam[] = [
    { role: "system", content: systemPrompt },
    ...recentHistory.map(
      (m) =>
        ({
          role: m.role,
          content: m.content,
        }) as OpenAI.ChatCompletionMessageParam,
    ),
    { role: "user", content: userMessage },
  ];

  // Attempt 1: primary model with full context
  const result = await callLLM(primaryModel, messages);
  if (result) return result;

  // Attempt 2: fallback model with reduced context + no history
  console.warn("[AI Chat] Primary model failed, trying fallback model...");
  const reducedContext = chatContextToPromptString({
    ...context,
    recentActivity: context.recentActivity.slice(0, 5),
  });
  const fallbackMessages: OpenAI.ChatCompletionMessageParam[] = [
    { role: "system", content: buildChatSystemPrompt(reducedContext) },
    { role: "user", content: userMessage },
  ];

  const fallbackResult = await callLLM(fallbackModel, fallbackMessages, true);
  if (fallbackResult) return fallbackResult;

  // Attempt 3: data-driven response (no LLM)
  console.warn("[AI Chat] All LLM attempts failed, using data-driven fallback");
  return buildDataDrivenResponse(context, userMessage);
}

async function callLLM(
  model: string,
  messages: OpenAI.ChatCompletionMessageParam[],
  useLegacyParam = false,
): Promise<string | null> {
  try {
    const params: OpenAI.ChatCompletionCreateParamsNonStreaming = {
      model,
      messages,
    };

    // gpt-4o family uses max_tokens, gpt-5 family uses max_completion_tokens
    if (useLegacyParam || model.startsWith("gpt-4")) {
      params.max_tokens = 2000;
    } else {
      params.max_completion_tokens = 2000;
    }

    const completion = await getOpenAI().chat.completions.create(params);

    const choice = completion.choices[0];
    const content = choice?.message?.content;

    if (choice?.finish_reason && choice.finish_reason !== "stop") {
      console.warn(
        `[AI Chat] finish_reason=${choice.finish_reason}, model=${model}, usage=${JSON.stringify(completion.usage)}`,
      );
    }

    if (!content || content.trim() === "") {
      console.error(
        `[AI Chat] Empty content. finish_reason=${choice?.finish_reason}, model=${model}, usage=${JSON.stringify(completion.usage)}`,
      );
      return null;
    }

    return content.trim();
  } catch (error) {
    console.error(`[AI Chat] ${model} call failed:`, error);
    return null;
  }
}

function buildDataDrivenResponse(ctx: ChatContext, query: string): string {
  const q = query.toLowerCase();

  // Profitability comparison
  if (q.includes("ìˆ˜ìµ") || q.includes("ë¹„êµ") || q.includes("profit")) {
    const sorted = [...ctx.projects]
      .filter((p) => p.realHourly !== null)
      .sort((a, b) => (b.realHourly ?? 0) - (a.realHourly ?? 0));

    if (sorted.length === 0) {
      return "ì•„ì§ ì‹¤ì œ ì‹œê¸‰ì´ ê³„ì‚°ëœ í”„ë¡œì íŠ¸ê°€ ì—†ì–´ìš”. ì‹œê°„ì„ ë” ê¸°ë¡í•´ë³´ì„¸ìš”!";
    }

    const best = sorted[0];
    const worst = sorted[sorted.length - 1];
    const lines = [`í”„ë¡œì íŠ¸ ìˆ˜ìµì„± ë¹„êµ ê²°ê³¼ì…ë‹ˆë‹¤.`];
    lines.push(
      `ê°€ì¥ ë†’ì€ ì‹¤ì œ ì‹œê¸‰: ${best.name} â€” ${best.currency} ${best.realHourly?.toLocaleString()}/h (ëª…ëª©: ${best.nominalHourly?.toLocaleString()}/h)`,
    );
    if (sorted.length > 1) {
      lines.push(
        `ê°€ì¥ ë‚®ì€ ì‹¤ì œ ì‹œê¸‰: ${worst.name} â€” ${worst.currency} ${worst.realHourly?.toLocaleString()}/h (ëª…ëª©: ${worst.nominalHourly?.toLocaleString()}/h)`,
      );
    }
    lines.push(
      `ì „ì²´ í‰ê·  ì‹¤ì œ ì‹œê¸‰: ${ctx.avgRealHourly?.toLocaleString() ?? "N/A"}/h`,
    );
    if (best.realHourly && best.nominalHourly) {
      const gap = Math.round(
        ((best.nominalHourly - best.realHourly) / best.nominalHourly) * 100,
      );
      if (gap > 0) {
        lines.push(
          `${best.name}ë„ ëª…ëª© ëŒ€ë¹„ ${gap}% ë‚®ì€ ì‹¤ì œ ì‹œê¸‰ì´ì—ìš”. ìˆ¨ì€ ë¹„ìš©ì„ í™•ì¸í•´ë³´ì„¸ìš”!`,
        );
      }
    }
    return lines.join("\n");
  }

  // Weekly summary
  if (q.includes("ì´ë²ˆ ì£¼") || q.includes("ìš”ì•½") || q.includes("week")) {
    const lines = [`ì´ë²ˆ ì£¼ ì‘ì—… ìš”ì•½ì…ë‹ˆë‹¤.`];
    lines.push(`ì´ ì‘ì—… ì‹œê°„: ${ctx.weeklyHours}ì‹œê°„`);
    lines.push(`í™œì„± í”„ë¡œì íŠ¸: ${ctx.totalActiveProjects}ê°œ`);
    if (ctx.activeAlertCount > 0) {
      lines.push(`í™œì„± ê²½ê³ : ${ctx.activeAlertCount}ê°œ â€” í™•ì¸ì´ í•„ìš”í•´ìš”!`);
    }

    const projectSummaries = ctx.projects
      .filter((p) => p.totalHours > 0)
      .slice(0, 5)
      .map((p) => `  - ${p.name}: ${p.totalHours}h (ì§„í–‰ ${p.progressPercent}%)`)
      .join("\n");
    if (projectSummaries) {
      lines.push(`í”„ë¡œì íŠ¸ë³„ í˜„í™©:\n${projectSummaries}`);
    }
    return lines.join("\n");
  }

  // Scope creep risk
  if (q.includes("ìŠ¤ì½”í”„") || q.includes("ìœ„í—˜") || q.includes("scope")) {
    const atRisk = ctx.projects.filter(
      (p) => p.hasActiveAlert || p.revisionPercent >= 30,
    );
    if (atRisk.length === 0) {
      return "í˜„ì¬ ìŠ¤ì½”í”„ í¬ë¦¬í”„ ìœ„í—˜ì´ ìˆëŠ” í”„ë¡œì íŠ¸ê°€ ì—†ì–´ìš”. ì˜ ê´€ë¦¬í•˜ê³  ê³„ì‹œë„¤ìš”! ğŸ‘";
    }
    const lines = [`ìŠ¤ì½”í”„ í¬ë¦¬í”„ ì£¼ì˜ í”„ë¡œì íŠ¸:`];
    for (const p of atRisk) {
      const reasons = [];
      if (p.hasActiveAlert) reasons.push("ê²½ê³  í™œì„±");
      if (p.revisionPercent >= 30) reasons.push(`ìˆ˜ì • ë¹„ìœ¨ ${p.revisionPercent}%`);
      lines.push(`- ${p.name}: ${reasons.join(", ")}`);
    }
    return lines.join("\n");
  }

  // Default: general summary
  const lines = [
    `í˜„ì¬ ${ctx.totalActiveProjects}ê°œ í”„ë¡œì íŠ¸ê°€ ì§„í–‰ ì¤‘ì´ì—ìš”.`,
    `ì´ ì‘ì—… ì‹œê°„: ${ctx.totalHours}ì‹œê°„, ì´ë²ˆ ì£¼: ${ctx.weeklyHours}ì‹œê°„`,
    `í‰ê·  ì‹¤ì œ ì‹œê¸‰: ${ctx.avgRealHourly?.toLocaleString() ?? "N/A"}/h`,
  ];
  if (ctx.activeAlertCount > 0) {
    lines.push(`${ctx.activeAlertCount}ê°œ ê²½ê³ ê°€ ìˆì–´ìš”. í™•ì¸í•´ë³´ì„¸ìš”!`);
  }
  lines.push(`ë” êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ í•´ì£¼ì‹œë©´ ìì„¸íˆ ë¶„ì„í•´ë“œë¦´ê²Œìš”.`);
  return lines.join("\n");
}
