import OpenAI from "openai";
import { buildChatContext, chatContextToPromptString } from "./chat-context";
import { buildChatSystemPrompt } from "./chat-prompt";

let _openai: OpenAI | null = null;

function getOpenAI(): OpenAI {
  if (!_openai) {
    _openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY! });
  }
  return _openai;
}

const FALLBACK_MESSAGE =
  "ì£„ì†¡í•©ë‹ˆë‹¤, ì¼ì‹œì ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆì–´ìš”. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.";

export async function generateChatResponse(
  userId: string,
  userMessage: string,
  conversationHistory: { role: "user" | "assistant"; content: string }[],
): Promise<string> {
  const context = await buildChatContext(userId);

  if (context.totalActiveProjects === 0 && context.recentActivity.length === 0) {
    return "ì•„ì§ í”„ë¡œì íŠ¸ë‚˜ ì‘ì—… ê¸°ë¡ì´ ì—†ì–´ìš”. ë¨¼ì € í”„ë¡œì íŠ¸ë¥¼ ë§Œë“¤ê³  ì‹œê°„ì„ ê¸°ë¡í•´ë³´ì„¸ìš”! ğŸ“";
  }

  const contextString = chatContextToPromptString(context);
  const systemPrompt = buildChatSystemPrompt(contextString);

  // Keep last 10 conversation pairs (20 messages)
  const recentHistory = conversationHistory.slice(-20);

  try {
    const model = process.env.LLM_MODEL_GENERATE || "gpt-4o-mini";

    const messages: OpenAI.ChatCompletionMessageParam[] = [
      { role: "system", content: systemPrompt },
      ...recentHistory.map(
        (m) =>
          ({
            role: m.role,
            content: m.content,
          }) as OpenAI.ChatCompletionMessageParam,
      ),
      { role: "user", content: userMessage },
    ];

    const completion = await getOpenAI().chat.completions.create({
      model,
      messages,
      max_completion_tokens: 500,
    });

    const content = completion.choices[0]?.message?.content;
    if (!content) throw new Error("LLM returned empty response");

    return content.trim();
  } catch (error) {
    console.error("[AI Chat] Generation failed:", error);
    return FALLBACK_MESSAGE;
  }
}
